{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n##https://www.kaggle.com/liananapalkova/simply-about-word2vec","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\nimport re\nimport sys\nimport copy\nimport torch \nimport numpy as np\nfrom scipy.sparse import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pyarrow as pa\n\nfrom keras.preprocessing import text, sequence\n\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset,DataLoader\n\n\nimport pandas as pd\n\nif not sys.warnoptions:    \n    import warnings\n    warnings.simplefilter(\"ignore\")","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\ntest_labels = pd.read_csv('//kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv')\ntrain = pd.read_csv('//kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('//kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['comment_text'][train['comment_text'].apply(len)==5000]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for i in train[156437:156438]['comment_text']: print(i)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(x):\n    x = re.sub(r'[^a-zA-Z0-9]',' ',x)\n    x = ' '.join(x.split())\n    x.lower()\n    return x","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'] = test['comment_text'].apply(clean)\ntrain['comment_text'] = train['comment_text'].apply(clean)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature engineering to prepare inputs for BERT....\n\nY = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\nX = train['comment_text'].values\n\n\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.33, random_state=42)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_x shape is {}' .format({X_train.shape}))\nprint('test_x shape is {}' .format({X_test.shape}))\nprint('train_y shape is {}' .format({y_train.shape}))","execution_count":9,"outputs":[{"output_type":"stream","text":"train_x shape is {(106912,)}\ntest_x shape is {(52659,)}\ntrain_y shape is {(106912, 6)}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=100000 #how many unique words to use (i.e num rows in embedding vector)\nmaxlen=150    # max number of words in a page content to use\nembed_size=300  # how big is each word vector","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tok=text.Tokenizer(num_words=max_features,lower=True)\n\ntok.fit_on_texts(list(X_train))\n\n\nX_train=tok.texts_to_sequences(X_train)\nX_test=tok.texts_to_sequences(X_test)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nX_test=sequence.pad_sequences(X_test,maxlen=maxlen)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tok.word_index\n#prepare embedding matrix\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(embedding_matrix)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"(100000, 300)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class text_dataset(Dataset):\n    def __init__(self,x,y, transform=None):\n        \n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        sentiments = self.y[index]\n        text = torch.LongTensor(self.x[index])               \n        return text, sentiments\n    \n    def __len__(self):\n        return len(self.x)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n\n\ntraining_dataset = text_dataset(X_train,y_train)\n\ntest_dataset = text_dataset(X_test,y_test)\n\ndataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=False),\n                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n                   }\ndataset_sizes = {'train':len(X_train),\n                'val':len(X_test)}","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self,feature_dim,step_dim,bias=True,**kwargs):\n        super(Attention,self).__init__(**kwargs)\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0 \n        \n        weight = torch.zeros(feature_dim,1)\n        nn.init.kaiming_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self,x,mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n        \n        eij = torch.mm(x.contiguous().view(-1,feature_dim),\n                      self.weight).view(-1,step_dim)\n        if self.bias:\n            eij = eij + self.b \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n        a = a / (torch.sum(a, 1 , keepdim = True) + 1e-10)\n        \n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input,1)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_LSTM_ATT(nn.Module):\n    \n    def __init__(self):\n        super(CNN_LSTM_ATT, self).__init__()\n        filter_sizes = [1,2,3,5]\n        num_filters = 36\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n        self.lstm1 = nn.LSTM(593, 128 , bidirectional = True, batch_first  = True)\n        self.lstm2 = nn.GRU(128*2,64,bidirectional=True,batch_first = True)\n        self.attention_layer = Attention(128, num_filters)\n        self.dropout = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(128,128)\n        self.fc2 = nn.Linear(128,6)\n        self.relu = nn.ReLU()\n     \n    def forward(self, x):\n        x = self.embedding(x)  \n        x = x.unsqueeze(1)  \n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n        x = torch.cat(x, 2) \n        #for i in x: print('shape after conv {}'.format(i.shape))\n        h_lstm, _ = self.lstm1(x)\n        h_lstm, _ = self.lstm2(h_lstm)\n        h_lstm_atten = self.attention_layer(h_lstm)\n        #x = self.dropout(x) \n        logit = self.relu(self.fc1(h_lstm_atten))\n        logit = self.fc2(logit)\n        return logit \n","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_Text_ATT(nn.Module):\n    \n    def __init__(self):\n        super(CNN_Text_ATT, self).__init__()\n        filter_sizes = [1,2,3,5]\n        num_filters = 36\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n        self.attention_layer = Attention(593, num_filters)\n        self.dropout = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(593,128)\n        self.fc2 = nn.Linear(128,6)\n        self.relu = nn.ReLU()\n     \n    def forward(self, x):\n        x = self.embedding(x)  \n        x = x.unsqueeze(1)  \n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n        x = torch.cat(x, 2) \n        #for i in x: print('shape after conv {}'.format(i.shape))\n        x = self.attention_layer(x)\n        x = self.dropout(x) \n        logit = self.relu(self.fc1(x))\n        logit = self.fc2(logit)\n        return logit ","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n    if sigmoid: y_pred = y_pred.sigmoid()\n#     return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = CNN_Text_ATT()\nmodel = CNN_LSTM_ATT()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\nlrlast = .01 #.001\nlrmain = 3e-5\n\noptim1 = torch.optim.Adam(model.parameters(),lrlast)\n\noptimizer_ft = optim1\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\ncriterion = nn.BCEWithLogitsLoss()\n\nprint(device)","execution_count":41,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler,num_epochs,attention):\n    model.train()\n    since = time.time()\n    print('starting')\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 100\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            F\n            beta_score_accuracy = 0.0\n            \n            micro_roc_auc_acc = 0.0\n            \n            \n            # Iterate over data.\n            for inputs, sentiments in dataloaders_dict[phase]:\n                \n                inputs = inputs.to(device) \n\n                sentiments = sentiments.to(device)\n            \n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    #outputs = torch.sigmoid(outputs)\n                    #print(outputs)\n                    loss = criterion(outputs,sentiments.float())\n                    \n                    if phase == 'train':\n                        \n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                \n                micro_roc_auc_acc +=  accuracy_thresh(outputs.view(-1,6),sentiments.view(-1,6))\n                \n                #print(micro_roc_auc_acc)\n\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n\n            \n            epoch_micro_roc_acc = micro_roc_auc_acc / dataset_sizes[phase]\n\n            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n            print('{} micro_roc_auc_acc: {:.4f}'.format( phase, epoch_micro_roc_acc))\n\n            if phase == 'val' and epoch_loss < best_loss:\n                print('saving with loss of {}'.format(epoch_loss),\n                      'improved over previous {}'.format(best_loss))\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(),'distilbert_model_weights.pth')\n\n        print()\n        time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(float(best_loss)))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n \nprint('done')","execution_count":42,"outputs":[{"output_type":"stream","text":"done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,10,Attention(491, 36))\n#model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,10)","execution_count":43,"outputs":[{"output_type":"stream","text":"starting\nEpoch 1/10\n----------\ntrain total loss: 0.0676 \ntrain micro_roc_auc_acc: 0.9761\nval total loss: 0.0602 \nval micro_roc_auc_acc: 0.9768\nsaving with loss of 0.060239845689204104 improved over previous 100\n\nEpoch 2/10\n----------\ntrain total loss: 0.0556 \ntrain micro_roc_auc_acc: 0.9795\nval total loss: 0.0532 \nval micro_roc_auc_acc: 0.9807\nsaving with loss of 0.053170775624067815 improved over previous 0.060239845689204104\n\nEpoch 3/10\n----------\ntrain total loss: 0.0489 \ntrain micro_roc_auc_acc: 0.9813\nval total loss: 0.0518 \nval micro_roc_auc_acc: 0.9808\nsaving with loss of 0.05175525527193139 improved over previous 0.053170775624067815\n\nEpoch 4/10\n----------\ntrain total loss: 0.0466 \ntrain micro_roc_auc_acc: 0.9820\nval total loss: 0.0513 \nval micro_roc_auc_acc: 0.9807\nsaving with loss of 0.051348642666484895 improved over previous 0.05175525527193139\n\nEpoch 5/10\n----------\ntrain total loss: 0.0448 \ntrain micro_roc_auc_acc: 0.9826\nval total loss: 0.0514 \nval micro_roc_auc_acc: 0.9807\n\nEpoch 6/10\n----------\ntrain total loss: 0.0424 \ntrain micro_roc_auc_acc: 0.9835\nval total loss: 0.0513 \nval micro_roc_auc_acc: 0.9804\nsaving with loss of 0.051346531835215044 improved over previous 0.051348642666484895\n\nEpoch 7/10\n----------\ntrain total loss: 0.0415 \ntrain micro_roc_auc_acc: 0.9839\nval total loss: 0.0515 \nval micro_roc_auc_acc: 0.9805\n\nEpoch 8/10\n----------\ntrain total loss: 0.0408 \ntrain micro_roc_auc_acc: 0.9841\nval total loss: 0.0518 \nval micro_roc_auc_acc: 0.9805\n\nEpoch 9/10\n----------\ntrain total loss: 0.0401 \ntrain micro_roc_auc_acc: 0.9844\nval total loss: 0.0517 \nval micro_roc_auc_acc: 0.9805\n\nEpoch 10/10\n----------\ntrain total loss: 0.0400 \ntrain micro_roc_auc_acc: 0.9845\nval total loss: 0.0517 \nval micro_roc_auc_acc: 0.9805\n\nTraining complete in 9m 55s\nBest val Acc: 0.051347\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test['comment_text']\nx_test=tok.texts_to_sequences(x_test)\nx_test=sequence.pad_sequences(x_test,maxlen=maxlen)\ny_test = np.zeros(x_test.shape[0]*6).reshape(x_test.shape[0],6)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = text_dataset(x_test,y_test)\nprediction_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n\ndef preds(model,test_loader):\n    predictions = []\n    for inputs, sentiment in test_loader:\n        inputs = inputs.to(device) \n        sentiment = sentiment.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n            outputs = torch.sigmoid(outputs)\n            predictions.append(outputs.cpu().detach().numpy().tolist())\n    return predictions","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = preds(model=model_ft1,test_loader=prediction_dataloader)\npredictions = np.array(predictions)[:,0]","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(predictions,columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate'])\ntest[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]=submission\nfinal_sub = test[['id','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\nfinal_sub.head()","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"                 id     toxic  severe_toxic   obscene    threat    insult  \\\n0  00001cee341fdb12  0.990284  2.254576e-01  0.951126  0.047459  0.832735   \n1  0000247867823ef7  0.000825  9.129903e-08  0.000117  0.000013  0.000076   \n2  00013b17ad220c46  0.000536  5.443849e-08  0.000089  0.000008  0.000050   \n3  00017563c3f7919a  0.000190  1.518447e-08  0.000035  0.000003  0.000017   \n4  00017695ad8997eb  0.000853  1.508437e-07  0.000123  0.000018  0.000083   \n\n   identity_hate  \n0       0.164205  \n1       0.000012  \n2       0.000007  \n3       0.000003  \n4       0.000016  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>0.990284</td>\n      <td>2.254576e-01</td>\n      <td>0.951126</td>\n      <td>0.047459</td>\n      <td>0.832735</td>\n      <td>0.164205</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>0.000825</td>\n      <td>9.129903e-08</td>\n      <td>0.000117</td>\n      <td>0.000013</td>\n      <td>0.000076</td>\n      <td>0.000012</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>0.000536</td>\n      <td>5.443849e-08</td>\n      <td>0.000089</td>\n      <td>0.000008</td>\n      <td>0.000050</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>0.000190</td>\n      <td>1.518447e-08</td>\n      <td>0.000035</td>\n      <td>0.000003</td>\n      <td>0.000017</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>0.000853</td>\n      <td>1.508437e-07</td>\n      <td>0.000123</td>\n      <td>0.000018</td>\n      <td>0.000083</td>\n      <td>0.000016</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub.to_csv('submissions.csv',index=False)#\nfinal_sub.head()","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"                 id     toxic  severe_toxic   obscene    threat    insult  \\\n0  00001cee341fdb12  0.990284  2.254576e-01  0.951126  0.047459  0.832735   \n1  0000247867823ef7  0.000825  9.129903e-08  0.000117  0.000013  0.000076   \n2  00013b17ad220c46  0.000536  5.443849e-08  0.000089  0.000008  0.000050   \n3  00017563c3f7919a  0.000190  1.518447e-08  0.000035  0.000003  0.000017   \n4  00017695ad8997eb  0.000853  1.508437e-07  0.000123  0.000018  0.000083   \n\n   identity_hate  \n0       0.164205  \n1       0.000012  \n2       0.000007  \n3       0.000003  \n4       0.000016  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>0.990284</td>\n      <td>2.254576e-01</td>\n      <td>0.951126</td>\n      <td>0.047459</td>\n      <td>0.832735</td>\n      <td>0.164205</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>0.000825</td>\n      <td>9.129903e-08</td>\n      <td>0.000117</td>\n      <td>0.000013</td>\n      <td>0.000076</td>\n      <td>0.000012</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>0.000536</td>\n      <td>5.443849e-08</td>\n      <td>0.000089</td>\n      <td>0.000008</td>\n      <td>0.000050</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>0.000190</td>\n      <td>1.518447e-08</td>\n      <td>0.000035</td>\n      <td>0.000003</td>\n      <td>0.000017</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>0.000853</td>\n      <td>1.508437e-07</td>\n      <td>0.000123</td>\n      <td>0.000018</td>\n      <td>0.000083</td>\n      <td>0.000016</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Play ground"},{"metadata":{},"cell_type":"markdown","source":"#### CONV WITH attention"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_sizes = [1,2,3,5]\n#filter_sizes = [5]\nnum_filters = 36\nembedding = nn.Embedding(max_features, embed_size)\n#embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\nembedding.weight.requires_grad = False\n#lstm = nn.LSTM(embed_size, 128 , bidirectional = True, batch_first  = True)\nlstm = nn.LSTM(embed_size, 128 , bidirectional = True, batch_first  = True)\nlstm2 = nn.GRU(128*2,64,bidirectional=True,batch_first = True)\nconvs2 = nn.Conv2d(1, num_filters, (5, embed_size))\nconvs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\nattention_layer = Attention(593,36)\ndropout = nn.Dropout(0.1)\n#fc1 = nn.Linear(len(filter_sizes)*num_filters, 6)\nfc1 = nn.Linear(593, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_play = train['comment_text'].values\nX_test_train=tok.texts_to_sequences(x_play[:10])\nX_test_train = sequence.pad_sequences(X_test_train,maxlen=maxlen)\nx= torch.LongTensor(X_test_train)\n\nx = embedding(x)\nx = x.unsqueeze(1) \n#x = [F.relu(conv(x)).squeeze(3) for conv in convs1]\nx= [F.relu(conv(x)).squeeze(3) for conv in convs1]\nfor i in x: print('shape after conv {}'.format(i.shape))\n\nx = torch.cat(x, 2) \nprint('shape after concat {}'.format(x.shape))\n\nx=attention_layer(x)\nprint('shape after attention {}'.format(x.shape))\nlogits = fc1(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CONV WITH LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_sizes = [1,2,3,5]\n#filter_sizes = [5]\nnum_filters = 36\nembedding = nn.Embedding(max_features, embed_size)\n#embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\nembedding.weight.requires_grad = False\n#lstm = nn.LSTM(embed_size, 128 , bidirectional = True, batch_first  = True)\nlstm = nn.LSTM(593, 128 , bidirectional = True, batch_first  = True)\nlstm2 = nn.GRU(128*2,64,bidirectional=True,batch_first = True)\nconvs2 = nn.Conv2d(1, num_filters, (5, embed_size))\nconvs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\nattention_layer = Attention(128,num_filters)\ndropout = nn.Dropout(0.1)\nrelu = nn.ReLU()\n#fc1 = nn.Linear(len(filter_sizes)*num_filters, 6)\nfc1 = nn.Linear(128, 128)\nrelu = nn.ReLU()\nout = nn.Linear(128,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_play = train['comment_text'].values\nX_test_train=tok.texts_to_sequences(x_play[:10])\nX_test_train = sequence.pad_sequences(X_test_train,maxlen=maxlen)\nx= torch.LongTensor(X_test_train)\nx = embedding(x)\nx = x.unsqueeze(1) \nx= [F.relu(conv(x)).squeeze(3) for conv in convs1]\n#for i in x: print('shape after conv {}'.format(i.shape))\nx = torch.cat(x, 2) \nh_lstm, _ =lstm(x)\nh_lstm, _ = lstm2(h_lstm)\nh_lstm_atten = attention_layer(h_lstm)\nprint(h_lstm_atten.shape)\nconc = relu(fc1(h_lstm_atten))\nout = out(conc)\nout.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}